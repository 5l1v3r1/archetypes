{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "\n",
    "# IMPORT LIBRARIES\n",
    "\n",
    "## OS\n",
    "import os\n",
    "\n",
    "# MANAGE\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import collections\n",
    "\n",
    "## FIT\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "## DECOMPOSITION\n",
    "from sklearn.decomposition import NMF\n",
    "from scipy.linalg import svd\n",
    "\n",
    "## PRESENTATION\n",
    "\n",
    "    ### Graphics\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "\n",
    "    ### Rich text \n",
    "from IPython.display import display, Markdown, Latex\n",
    "\n",
    "    ### Widgets\n",
    "import ipywidgets as widgets\n",
    "\n",
    "\n",
    "## I/O\n",
    "import zipfile\n",
    "import requests\n",
    "import pickle\n",
    "\n",
    "## DOWNLOAD\n",
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "\n",
    "## NLP\n",
    "import nltk\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "from nltk.corpus import stopwords\n",
    "set(stopwords.words('english'))\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "class DownloadProgressBar(tqdm):\n",
    "    def update_to(self, b=1, bsize=1, tsize=None):\n",
    "        if tsize is not None:\n",
    "            self.total = tsize\n",
    "        self.update(b * bsize - self.n)\n",
    "\n",
    "def download_url(url, output_path):\n",
    "    with DownloadProgressBar(unit='B', unit_scale=True,\n",
    "                             miniters=1, desc=url.split('/')[-1]) as t:\n",
    "        urllib.request.urlretrieve(url, filename=output_path, reporthook=t.update_to)\n",
    "\n",
    "\n",
    "## CREATE PATHS / DIRECTORIES \n",
    "\n",
    "# path to home directory (the location of this file)\n",
    "path0 = !pwd\n",
    "path = path0.n\n",
    "\n",
    "os.chdir(path)\n",
    "\n",
    "#Check if directories exist - creat directories if needed\n",
    "\n",
    "paths = {}\n",
    "paths['data'] = './data'\n",
    "paths['census'] = paths['data'] + '/census'\n",
    "paths['onet'] = paths['data'] + '/onet'\n",
    "\n",
    "[os.makedirs(pth, exist_ok=True) for pth in paths.values()]\n",
    "\n",
    "\n",
    "## GENERAL FUNCTIONS \n",
    "### NORMALIZATION\n",
    "# Statistic normalization - subtract mean, scale by standard deviation\n",
    "def norm_stat(vec, weights = False):\n",
    "    '''\n",
    "    Normalizes a vector v-v.mean())/v.std() \n",
    "    '''\n",
    "    if weights:\n",
    "        return  np.mean(abs(vec - vec.mean()))  \n",
    "    return (vec-vec.mean())/vec.std()\n",
    "\n",
    "# Algebraic normalization - dot product\n",
    "def norm_dot(vec, weights = False):\n",
    "    '''\n",
    "    Normalizes the columns of a DataFrame (dot product)\n",
    "    '''\n",
    "    if weights:\n",
    "        return  np.sqrt(vec @ vec)\n",
    "    \n",
    "    return vec / np.sqrt(vec @ vec)\n",
    "\n",
    "# Algebraic normalization - dot product\n",
    "def norm_sum(vec, weights = False):\n",
    "    '''\n",
    "    Normalizes the columns of a DataFrame (dot product)\n",
    "    '''\n",
    "    if weights:\n",
    "        return  vec.sum()\n",
    "    \n",
    "    return vec / vec.sum()\n",
    "\n",
    "# \n",
    "# Scaled Normalization -\n",
    "def scale(vec, weights = False):\n",
    "    stop_divide_by_zero = 0.00000001\n",
    "    if weights:\n",
    "        return (vec.max()-vec.min() + stop_divide_by_zero)\n",
    "    return (vec-vec.min())/(vec.max()-vec.min() + stop_divide_by_zero)\n",
    "\n",
    "\n",
    "\n",
    "### SELECTION \n",
    "def is_string(a):\n",
    "    '''\n",
    "    typically used for Boolean masking in Pandas, e.g.  \n",
    "               df[df['column'].apply(is_string)] \n",
    "    returns all rows in df where df['column'] has a string value   \n",
    "    '''\n",
    "    return isinstance(a,str)\n",
    "\n",
    "\n",
    "## CLASSES\n",
    "\n",
    "### DATA DICTIONARY \n",
    "\n",
    "class    Datadic: \n",
    "    def __init__(self):\n",
    "        # Set up FIPS CODES for states and regions\n",
    "        fips_codes_exists = os.path.isfile('data/state-geocodes-v2016.xls')\n",
    "        if not fips_codes_exists:\n",
    "            print('*** FIPS State Geocodes is missing. Downloading from Census...')\n",
    "            # !curl -o ./data/state-geocodes-v2016.xls -O https://www2.census.gov/programs-surveys/popest/geographies/2016/state-geocodes-v2016.xls\n",
    "            download_url('https://www2.census.gov/programs-surveys/popest/geographies/2016/state-geocodes-v2016.xls','./data/state-geocodes-v2016.xls')\n",
    "            print('*** Complete.')\n",
    "\n",
    "        self.dfips = pd.read_excel('data/state-geocodes-v2016.xls')[5:] #five first rows are comment\n",
    "        self.name_to_fips = self.dfips.set_index('Unnamed: 3')['Unnamed: 2'].to_dict()\n",
    "        self.fips_to_name = self.dfips.set_index('Unnamed: 2')['Unnamed: 3'].to_dict()\n",
    "        self.translate_fips = {**self.name_to_fips,**self.fips_to_name}\n",
    "        \n",
    "        self.state_to_abbrev = {'Alabama': 'AL','Alaska': 'AK','Arizona': 'AZ','Arkansas': 'AR','California': 'CA','Colorado': 'CO',\n",
    "                'Connecticut': 'CT','Delaware': 'DE','District of Columbia': 'DC','Florida': 'FL','Georgia': 'GA','Hawaii': 'HI',\n",
    "                'Idaho': 'ID','Illinois': 'IL','Indiana': 'IN','Iowa': 'IA','Kansas': 'KS','Kentucky': 'KY','Louisiana': 'LA',\n",
    "                'Maine': 'ME','Maryland': 'MD','Massachusetts': 'MA','Michigan': 'MI','Minnesota': 'MN','Mississippi': 'MS','Missouri': 'MO',\n",
    "                'Montana': 'MT','Nebraska': 'NE','Nevada': 'NV','New Hampshire': 'NH','New Jersey': 'NJ','New Mexico': 'NM',\n",
    "                'New York': 'NY','North Carolina': 'NC','North Dakota': 'ND','Ohio': 'OH','Oklahoma': 'OK','Oregon': 'OR',\n",
    "                'Pennsylvania': 'PA','Rhode Island': 'RI','South Carolina': 'SC','South Dakota': 'SD','Tennessee': 'TN','Texas': 'TX',\n",
    "                'Utah': 'UT','Vermont': 'VT','Virginia': 'VA','Washington': 'WA','West Virginia': 'WV','Wisconsin': 'WI','Wyoming': 'WY'}\n",
    "        self.abbrev_to_state = {v: k for k, v in self.state_to_abbrev.items()}\n",
    "        self.translate_state_abbrev = {**self.state_to_abbrev,**self.abbrev_to_state}\n",
    "        \n",
    "        #Set up CENSUS/ACS PUMS DATA DICTIONARY\n",
    "        pums_datadic_exists = os.path.isfile('./data/census/PUMS_Data_Dictionary.csv')\n",
    "        if not pums_datadic_exists:\n",
    "            print('*** Census ACS/PUMS Data Dictionary is missing. Downloading from Census...')\n",
    "            # !curl -o ./data/census/PUMS_Data_Dictionary.csv -O https://www2.census.gov/programs-surveys/acs/tech_docs/pums/data_dict/PUMS_Data_Dictionary_2017.csv\n",
    "            download_url('https://www2.census.gov/programs-surveys/acs/tech_docs/pums/data_dict/PUMS_Data_Dictionary_2017.csv','./data/census/PUMS_Data_Dictionary.csv')\n",
    "            print('*** Complete.')\n",
    "        self.census = pd.read_csv(\"data/census/PUMS_Data_Dictionary.csv\").drop_duplicates()\n",
    "        self.census_variable_definitions = self.census.groupby('RT').first()['Record Type'].to_dict()      \n",
    "            \n",
    "    def fips(self,name_or_fipsnr):\n",
    "        nn = datadic.abbrev_to_state.get(name_or_fipsnr,name_or_fipsnr)\n",
    "        return self.translate_fips.get(nn)\n",
    "    \n",
    "    def state_abbrev(self,name_or_abbrev):\n",
    "        return self.translate_state_abbrev.get(name_or_abbrev)\n",
    "    \n",
    "    def census_def(self,variable_code):\n",
    "        return self.census_variable_definitions.get(variable_code)\n",
    "\n",
    "    # All definitions containing a select string\n",
    "    def clk(self,census_col,search_string):\n",
    "        return self.census[census_col].fillna('nan').apply(lambda x: search_string.lower() in x.lower())\n",
    "    \n",
    "    def census_about(self,search_string):\n",
    "        return self.census[self.clk('Record Type',search_string) | self.clk('Unnamed: 6',search_string) ]\n",
    "    \n",
    "datadic = Datadic()\n",
    "\n",
    "\n",
    "class Onet:\n",
    "    '''\n",
    "    Onet() is an object based on the Onet labor market database. \n",
    "\n",
    "    my_onet.source  - string: URL for importing the onet database from source\n",
    "    my_onet.path    - string: local path to the directory where the onet database is stored. (set in 'paths' dictionary)\n",
    "    my_onet.name    - string: the prefix of stored files, e.g. zipped DB:      path + '/'+ name +'.zip'\n",
    "    my_onet.toc()   - function: returns table of contents for onet database\n",
    "\n",
    "   # Data in Onet database\n",
    "    my_onet.data()\n",
    "                    - function: returns dataset named by label.\n",
    "    my_onet.matrix() \n",
    "                    - function: returns onet dataset in matrix form\n",
    "    my_onet.n_matrix() \n",
    "                    - function: normalized onet matrix\n",
    " \n",
    "   \n",
    "    '''\n",
    "    def __init__(self,path = paths['onet'], name = 'onet', source = 'https://www.onetcenter.org/dl_files/database/db_23_3_excel.zip'):\n",
    "        \n",
    "        self.path = path\n",
    "        self.name = name\n",
    "        self.source = source\n",
    "        self.dataset = {}\n",
    "        self.matrix_dic = {}\n",
    "\n",
    "        self.socp_titles = self.data('Alternate Titles',socp_shave = 8)[['SOCP_shave','Title']].drop_duplicates()\n",
    "        \n",
    "        zip_file = path + '/'+ name +'.zip'\n",
    "        onet_exists = os.path.isfile(zip_file)\n",
    "        if not onet_exists:\n",
    "            print('*** Onet database does not exist. Downloading from Onet...')\n",
    "            #shcmd = 'curl -o '+zip_file+' -O '+source\n",
    "            #!$shcmd'\n",
    "            download_url(source,zip_file)\n",
    "            print('*** Complete.')\n",
    "        \n",
    "        self.zip = zipfile.ZipFile(zip_file)\n",
    "        self.tocdf = self.make_toc()\n",
    "    \n",
    "    def make_toc(self,sep ='.'):\n",
    "        '''\n",
    "        Creates table of contents for Onet Database, returns as my_onet.tocdf (dataframe)\n",
    "        '''\n",
    "        nlst = np.array(self.zip.namelist())\n",
    "        dr = nlst[0]\n",
    "        nl = pd.DataFrame(nlst)\n",
    "        self.tocdf = pd.DataFrame(nl[0].apply(lambda x: np.char.split(x.replace(dr,''),sep = '.'))[1:].to_dict(),\n",
    "                                    index = ['name','extension']).T\n",
    "        return self.tocdf\n",
    "    \n",
    "    def toc(self, name_contains= False, extension = False):\n",
    "        '''\n",
    "        Returns table of contents for Onet Database (dataframe) masked by string and/or extension\n",
    "        '''\n",
    "        selection = self.tocdf\n",
    "        if extension:\n",
    "            selection = selection[selection['extension'] == extension  ]['name']\n",
    "        if name_contains:\n",
    "            search_string = name_contains\n",
    "            selection = selection[selection['name'].apply(lambda x: search_string.lower() in x.lower())]['name']\n",
    "        return selection\n",
    "         \n",
    "    \n",
    "    def data(self,label, socp_shave = 6):\n",
    "        '''\n",
    "        Returns onet dataset named 'label'\n",
    "        '''\n",
    "        # If dataframe in dictionary:  \n",
    "        if label in self.dataset.keys():\n",
    "            df = self.dataset[label]\n",
    "            df['SOCP_shave'] = df['O*NET-SOC Code'].apply(lambda x: x.replace('.','').replace('-','')).apply(lambda x: x[:socp_shave])\n",
    "            return self.dataset[label]\n",
    "\n",
    "        # If dataframe NOT in dictionary: \n",
    "        # If pickled dataframe does not exist, create from zipped excel\n",
    "        # Read pickled dataframe into dictionary\n",
    "        pkl_name = self.path +'/'+ self.name +'_'+ label +'.pkl'\n",
    "        pkl_exists = os.path.isfile(pkl_name)\n",
    "\n",
    "        if not pkl_exists:\n",
    "            print('*** '+label+'.pkl does not exist. Creating...')\n",
    "            xlsx_name = self.zip.namelist()[0] + label +'.xlsx'\n",
    "            pd.read_excel(self.zip.extract(xlsx_name)).to_pickle(pkl_name)\n",
    "            print('*** Complete.')\n",
    "        df = pd.read_pickle(pkl_name)\n",
    "        df['SOCP_shave'] = df['O*NET-SOC Code'].apply(lambda x: x.replace('.','').replace('-','')).apply(lambda x: x[:socp_shave])\n",
    "        self.dataset[label] = df                              \n",
    "        return self.dataset[label]\n",
    "    \n",
    "    def grpby(self, label, columns = ['Scale Name','Element Name'], data_value = 'Data Value', scale_name = 'Level'):\n",
    "        grp = pd.DataFrame(self.data(label).copy().groupby(columns).apply(lambda x: x[data_value].values))\n",
    "        return grp\n",
    "\n",
    "    def matrix(self,label, xx = 'Element Name',  yy = 'SOCP_shave',socp_shave = 6 , data_value = 'Data Value', scale_name = 'Level',\n",
    "                show = 'mean', norm = False):\n",
    "        '''\n",
    "        Converts onet dataset into a matrix \n",
    "        xx          - matrix columns\n",
    "        yy          - matrix index\n",
    "        scale_name  - value category \n",
    "        data_value  - data values\n",
    "        socp_shave  - number of digits in 'shaved' SOCP number\n",
    "        show        - output matrix shows 'mean'(default), 'std' or 'count' (relevant for groupby socp_shave)\n",
    "        norm        - columns are normalized: 'norm = norm_dot'  [ col/sqrt(col@col) ] \n",
    "                                              'norm = norm_stat' [ (col - col.mean) / col.std ] \n",
    "                                         \n",
    "        '''\n",
    "        if not (label,xx,yy,socp_shave,data_value,scale_name,norm) in self.matrix_dic.keys():\n",
    "            print('*** Onet matrix not in dictionary. Constructing....')\n",
    "            columns = ['Scale Name',yy,xx] # Default columns\n",
    "            grpb = self.data(label,socp_shave = socp_shave).groupby(columns)\n",
    "            mat_mean   = grpb.mean().loc[scale_name][data_value].unstack()\n",
    "            mat_std    = grpb.std().loc[scale_name][data_value].unstack()\n",
    "            mat_count = grpb.count().loc[scale_name][data_value].unstack().T.iloc[[0]].T\n",
    "            if norm:\n",
    "                w = mat_mean.apply(lambda col: norm(col,weights = True) )\n",
    "                mat_mean = mat_mean.apply(norm)\n",
    "                mat_std = mat_std/w\n",
    "            self.matrix_dic[label,xx,yy,socp_shave,data_value,scale_name,norm] = {\n",
    "                'mean':mat_mean,'std':mat_std,'count':mat_count}\n",
    "            print('*** Complete')\n",
    "        return self.matrix_dic[label,xx,yy,socp_shave,data_value,scale_name,norm][show]\n",
    "    \n",
    "       \n",
    "# Instantiate Onet() as 'onet'\n",
    "onet = Onet()            \n",
    "\n",
    "#%%    \n",
    "\n",
    "class   Census:\n",
    "    '''\n",
    "    Census() is an object containing census ACS/PUMS data \n",
    "    my_census.source    - string: URL for importing the census data from US Census online\n",
    "    my_census.path      - string: local path to the directory where the census data is stored. (set in 'paths' dictionary)\n",
    "    my_census.name      - string: the prefix of stored files, e.g. pickle:      path + '/'+ name +'.pkl'\n",
    "\n",
    "    my_census.data(self,state, socp_shave = n)\n",
    "                        - function: returns the acs/pums for 'state', with an SOCP number of 'n' digits (default n=6)\n",
    "    my_census.dataset   - dataframe: acs/pums data for a state (state abbreviation used for naming)\n",
    "    my_census.import_from_source(self,state)\n",
    "                        - function: imports data for 'state' from my_census.source and converts to pickled dataframe \n",
    "\n",
    "\n",
    "    # my_census.state    – String: name of state, e.g. 'California', or abbreviation, e.g. 'CA'\n",
    "    # my_census.data     – DataFrame: imported census ACS/PUMS data (from pickle)\n",
    "    # my_census.workers  – DataFrame: people fulfilling 'workers' criteria (see below)\n",
    "    # my_census.occupations         – DataFrame: Occupations of the workers (groupby SOCP-number)\n",
    "    \n",
    "    # my_census.workers_occupations(age_low = 40, age_high = 65, std_max = 0.5, socp_granularity = 5):\n",
    "    #      - Function: populates my_census.workers / .occupations according to criteria\n",
    "    #          – Workers: age_low (default 40)  and age_high (default 65)\n",
    "    #          – Occupations: socp_granularity (default 5) ; the length of the SOCP number, default 5 digits.\n",
    "    \n",
    "    '''\n",
    "\n",
    "    def __init__(self,path = paths['census'], \n",
    "                 name = 'census', \n",
    "                 source = 'https://www2.census.gov/programs-surveys/acs/data/pums/2017/5-Year/'):\n",
    "        self.path = path\n",
    "        self.name = name\n",
    "        self.source = source\n",
    "        self.dataset = {}\n",
    "        \n",
    "    \n",
    "    def data(self,state, socp_shave = 6):\n",
    "        '''\n",
    "        READ CENSUS/ACS PUMS DATABASE. Search order: Dictionary, Pickle; Create dictionary/pickle if non-existent.\n",
    "        socp_shave  : number of digits in 'shaved' SOCP number\n",
    "        '''\n",
    "        state_abbr = datadic.state_to_abbrev.get(state,state)\n",
    "        pkl_name = self.path +'/'+ self.name +'_'+ state_abbr +'.pkl'\n",
    "        pkl_exists = os.path.isfile(pkl_name)\n",
    "        if not pkl_exists:\n",
    "            self.import_from_source(state)\n",
    "        df = pd.read_pickle(pkl_name)\n",
    "        df['SOCP_shave'] = df['SOCP'].apply(lambda x: x[:socp_shave].replace('X','0') if type(x)==str else x)\n",
    "        self.dataset[state_abbr] = df\n",
    "        return self.dataset[state_abbr]\n",
    "\n",
    "\n",
    "    # Create and execute shell command fetching state census zip-file \n",
    "    def import_from_source(self,state):\n",
    "        '''\n",
    "        Imports ACS/PUMS dataset from US Census online, URL: my_census.source\n",
    "        '''\n",
    "        print('*** Downloading '+state+' ACS/PUMS dataset from US Census...')\n",
    "        state_abbr = datadic.state_to_abbrev.get(state,state)\n",
    " #       shcmd = \"curl -o \"+self.path+'/'+self.name+\"_tmp.zip -O \"+ self.source +self.state_zipfile_name(state_abbr)\n",
    " #       ! $shcmd\n",
    "        download_url(self.source +self.state_zipfile_name(state_abbr),self.path+'/'+self.name+\"_tmp.zip\")\n",
    "        print('*** Reformatting...')\n",
    "        with zipfile.ZipFile(self.path+'/'+self.name+\"_tmp.zip\", 'r') as zipObj:\n",
    "            zipObj.extractall(self.path)\n",
    "        csv_filename = self.path+'/psam_p'+datadic.fips(state)+'.csv'\n",
    "        pkl_filename = self.path+'/'+self.name+'_'+state_abbr+'.pkl'\n",
    "        pd.read_csv(csv_filename).to_pickle(pkl_filename)\n",
    "        ! rm $csv_filename \n",
    "        print('*** Complete.')\n",
    "        return \n",
    "\n",
    "    def state_zipfile_name(self,state_abbr):\n",
    "        '''\n",
    "        Input: State name abbreviation. Returns census-convention name of zipped csv-file\n",
    "        '''\n",
    "        return 'csv_p'+state_abbr.lower()+'.zip'\n",
    "    \n",
    "\n",
    "# Instantiate Census() as 'census'\n",
    "census = Census()\n",
    "\n",
    "# # Xy - matrix\n",
    "class MakeXy:\n",
    "    '''\n",
    "    MakeXy is an object containing a combination of census and onet data\n",
    "    \n",
    "    my_makeXy = MakeXy(my_census,my_onet)\n",
    "    \n",
    "    my_makeXy.Xy     : Merges my_census and my_onet with 'SOCP_shave' (occupational code)\n",
    "                                as common variable and my_census[y_label] as target variable, and groups: \n",
    "                                my_make.Xy(y_label) = merged.groupby('SOCP_shave').sum() \n",
    "    my_makeXy.X      : X-matrix / independent variables\n",
    "    my_makeXy.y      : y-matrix / target\n",
    "    \n",
    "    '''\n",
    "    def __init__(self,census,onet,y_label = 'fte'):\n",
    "        self.census  = census\n",
    "        self.onet    = onet\n",
    "        self.y_label = y_label\n",
    "        merged       = pd.merge(self.census[['SOCP_shave',self.y_label]],self.onet,\n",
    "                         left_on = 'SOCP_shave',right_index=True)\n",
    "        self.Xy =  merged.groupby('SOCP_shave').sum()\n",
    "        self.X  =  self.Xy.drop(self.y_label, axis =1)\n",
    "        self.y  =  self.Xy[self.y_label]\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# # MATRIX-FACTORIZATION: DIMENSIONALITY REDUCTION & ARCHETYPING\n",
    "\n",
    "# ## CLUSTER FEATURES INTO OCCUPATION CATEGORIES\n",
    "# ## Use non-zero matrix factorization for clustering\n",
    "# ## Use singular value decomposition first state for determining overall similarity\n",
    "\n",
    "\n",
    "class Archetypes:\n",
    "    '''\n",
    "    Archetypes: Performs NMF of order n on X and stores the result as attributes. \n",
    "    Archetypes are normalized: cosine similarity a(i) @ a(i) = 1. \n",
    "    Atributes:\n",
    "        my_archetypes.n         - order / number of archetypes\n",
    "        my_archetypes.X         - input matrix\n",
    "        \n",
    "        my_archetypes.model     - NMF model \n",
    "        my_archetypes.w         - NMF w-matrix \n",
    "        my_archetypes.h         - NMF h-matrix\n",
    "        \n",
    "        my_archetypes.o         - occupations x archetypes matrix (from w-matrix)\n",
    "        my_archetypes.on        - occupations x normalized archetypes matrix (from w-matrix) - SOCP number as index. \n",
    "        my_archetypes.occ       - occupations x normalized archetypes matrix - Occupation names as index\n",
    "        \n",
    "        my_archetypes.f         - features x archetypes matrix (from h-matrix)\n",
    "        my_archetypes.fn        - features x normalized archetypes matrix\n",
    "        \n",
    "    '''\n",
    "    def __init__(self,X,n,norm = norm_dot):\n",
    "        self.n = n\n",
    "        self.X = X\n",
    "        self.model = NMF(n_components=n, init='random', random_state=0, max_iter = 1000, tol = 0.0000001)\n",
    "        self.w = self.model.fit_transform(self.X)\n",
    "        self.o = pd.DataFrame(self.w,index=self.X.index)\n",
    "        self.on = self.o.T.apply(norm).T\n",
    "        self.occ = self.on.copy()\n",
    "        self.occ['Occupations'] = self.occ.index\n",
    "#        self.occ['Occupations'] = self.occ['Occupations'].apply(onet_socp_name)\n",
    "        self.occ = self.occ.set_index('Occupations')\n",
    "        self.h = self.model.components_\n",
    "        self.f = pd.DataFrame(self.h,columns=X.columns)\n",
    "        self.fn =self.f.T.apply(norm).T\n",
    "        self.plot_occupations_dic ={}\n",
    "        self.plot_features_dic ={}\n",
    "\n",
    "        \n",
    "    def plot_features(self,fig_scale = (1,3.5),metric='cosine', method = 'single',vertical = False): \n",
    "        '''\n",
    "        Plot Archetypes as x and features as y. \n",
    "        Utilizes Seaborn Clustermap, with hierarchical clustering along both axes. \n",
    "        This clusters features and archetypes in a way that visualizes similarities and diffferences\n",
    "        between the archetypes. \n",
    "        \n",
    "        Archetypes are normalized (cosine-similarity): dot product archetype[i] @ archetype[i] = 1.\n",
    "        The plot shows intensities (= squared feature coefficients) so that the sum of intensities = 1.  \n",
    "\n",
    "        fig_scale: default values (x/1, y/3.5) scales the axes so that all feature labels are included in the plot.\n",
    "        \n",
    "        For other hyperparameters, see seaborn.clustermap\n",
    "     \n",
    "        '''\n",
    "        param = (fig_scale,metric,method,vertical)\n",
    "        if param in self.plot_features_dic.keys():\n",
    "            fig = self.plot_features_dic[param]\n",
    "            return fig.fig\n",
    "\n",
    "        df = np.square(self.fn)\n",
    "\n",
    "        if vertical:\n",
    "            fig = sns.clustermap(df.T,robust = True, z_score=1,figsize=(\n",
    "                self.n/fig_scale[0],self.X.shape[1]/fig_scale[1]),method = method,metric = metric)        \n",
    "        else: # horizontal\n",
    "            fig = sns.clustermap(df,robust = True, z_score=0,figsize=(\n",
    "                self.X.shape[1]/fig_scale[1],self.n/fig_scale[0]),method = method,metric = metric)        \n",
    "        self.features_plot = fig\n",
    "        return fig\n",
    "\n",
    "\n",
    "    def plot_occupations(self,fig_scale = (1,3.5),metric='cosine', method = 'single',vertical = False):\n",
    "        '''\n",
    "        Plot Archetypes as x and occupations as y. \n",
    "        Utilizes Seaborn Clustermap, with hierarchical clustering along both axes. \n",
    "        This clusters occupations and archetypes in a way that visualizes similarities and diffferences\n",
    "        between the archetypes. \n",
    "        \n",
    "        Occupations are normalized (cosine-similarity): dot product occupation[i] @ occupation[i] = 1.\n",
    "        The plot shows intensities (= squared feature coefficients) so that the sum of intensities = 1.  \n",
    "\n",
    "        fig_scale: default values (x/1, y/3.5) scales the axes so that all feature labels are included in the plot.\n",
    "        \n",
    "        For other hyperparameters, see seaborn.clustermap\n",
    "     \n",
    "        '''\n",
    "        param = (fig_scale,metric,method,vertical)\n",
    "        if param in self.plot_occupations_dic.keys():\n",
    "            fig = self.plot_occupations_dic[param]\n",
    "            #return\n",
    "            return fig.fig\n",
    "\n",
    "        df = np.square(self.occ)\n",
    "        if vertical:\n",
    "            fig = sns.clustermap(df, figsize=(\n",
    "                self.n/fig_scale[0],self.X.shape[0]/fig_scale[1]),method = method,metric = metric)\n",
    "        else: # horizontal\n",
    "            fig = sns.clustermap(df.T, figsize=(\n",
    "                self.X.shape[0]/fig_scale[1],self.n/fig_scale[0]),method = method,metric = metric)\n",
    "        self.plot_occupations_dic[param] = fig\n",
    "        #return\n",
    "        return fig.fig\n",
    "\n",
    "        \n",
    "class Xfit:\n",
    "    '''\n",
    "    Xfit is a 'fit-as-an-object' solution:\n",
    "        my_fit = Xfit(X,y,Xsamples=False, my_regressor, itr, xval) \n",
    "            does the following:\n",
    "            0. SAMPLES X - unless Xsamples is 'False' [default value], X is replaced by n random samples of itself  \n",
    "            1. SPLITS X and y into test and training sets. \n",
    "            2. FITS a cross-validation, slicing the training data into 'xval' slices : cross_validate(regressor,X_train.values, y_train.values, cv=xval) \n",
    "            3. BOOTSTRAPS: Repeats (1-2) 'itr' number of times\n",
    "            4. RETURNS RESULTS as attributes:\n",
    "                my_fit.X          – List: The original X input data\n",
    "                my_fit.itr        – Number of iterations / fits\n",
    "                my_fit.y          – List: The original y input data\n",
    "                my_fit.xval       – Number of slices in the cross validation\n",
    "                my_fit.fit        – Dictionary: the 'itr' number of cross-validated fits, including estimators\n",
    "                my_fit.y_test     – Dictionary: the y_test (list) for each fit\n",
    "                my_fit.y_predict  – Dictionary: the predicted y for each fit\n",
    "                my_fit.scores     – Pandas.DataFrame: validation scores for all fits \n",
    "                my_fit.score      – Dictionary: the mean score and standard deviation. \n",
    "                my_fit.features_importances        – Dictionary: feature_importances for all fits (for estimators with '.feature_importance_' as an attribute )\n",
    "                my_fit.feature_importance          – Pandas.DataFrame: the average feature importances and standard deviations.           \n",
    "    '''\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.model_selection import cross_validate\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    from sklearn import linear_model\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.ensemble import GradientBoostingRegressor\n",
    "    import xgboost as xgb  \n",
    "\n",
    "    def __init__(self,X,y,Xsamples=False,regressor = xgb.XGBRegressor(),itr = 10, xval = 3):      \n",
    "        # FITTING\n",
    "        n = xval  \n",
    "        feature_names = X.columns\n",
    "        res = {}\n",
    "        ypred = {}\n",
    "        ytest = {}\n",
    "        scor = {}\n",
    "        feat_imp = {}       \n",
    "        for i in range(itr):\n",
    "            if Xsamples:\n",
    "                X_train, X_test, y_train, y_test = train_test_split(X.sample(Xsamples,axis=1), y, test_size=0.2)\n",
    "            else:\n",
    "                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "            res_xboo = cross_validate(regressor,X_train.values, y_train.values, cv=n, return_estimator=True)\n",
    "            ytest[i] = y_test\n",
    "            res[i] = res_xboo\n",
    "            ypred[i] = [res_xboo['estimator'][j].predict(X_test.values) for j in range(n)]\n",
    "            scor[i] = [res_xboo['estimator'][j].score(X_test.values,y_test.values) for j in range(n)]\n",
    "            feat_imp[i] = [res_xboo['estimator'][j].feature_importances_ for j in range(n)]\n",
    "        scor_tot = np.concatenate(np.array(list(scor.values())))\n",
    "        feat_tot = pd.concat([pd.DataFrame(feat_imp[i]) for i in range(itr)])\n",
    "#       # feat_tot.columns = X.columns\n",
    "        feat_tot.reset_index(inplace=True,drop = True)\n",
    "        feat_mean = pd.concat([feat_tot.mean(),feat_tot.std()],axis=1)\n",
    "        feat_mean.columns = ['mean','std']\n",
    "        feat_mean['ratio'] = feat_mean['std']/feat_mean['mean']      \n",
    "        # STORING RESULTS AS ATTRIBUTES\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.fit = res\n",
    "        self.y_predict = ypred\n",
    "        self.y_test = ytest\n",
    "        self.scores = pd.DataFrame(scor).T\n",
    "        self.score = {'mean':scor_tot.mean(), 'std':scor_tot.std()}\n",
    "        self.feature_importances = feat_imp\n",
    "        self.feature_importance = feat_mean.sort_values('mean',ascending=False)\n",
    "        self.itr =itr\n",
    "        self.xv = xval\n",
    "\n",
    "\n",
    "\n",
    "class Svd:\n",
    "    ''''\n",
    "    Singular value decomposition-as-an-object\n",
    "        my_svd = Svd(X) returns\n",
    "        my_svd.u/.s/.vt – U S and VT from the Singular Value Decomposition (see manual)\n",
    "        my_svd.f        – Pandas.DataFrame: f=original features x svd_features\n",
    "        my_svd.o        - Pandas.DataFrame: o=occupations x svd_features\n",
    "        my_svd.volume(keep_volume) \n",
    "                        - collections.namedtuple ('dotted dicionary'): \n",
    "                          Dimensionality reduction. keeps 'keep_volume' of total variance\n",
    "                          \n",
    "                          \n",
    "    '''\n",
    "    def __init__(self,X):\n",
    "        self.u,self.s,self.vt = svd(np.array(X))\n",
    "        self.f = pd.DataFrame(self.vt,columns=X.columns)\n",
    "        self.o = pd.DataFrame(self.u,columns=X.index)\n",
    "        \n",
    "    def volume(self,keep_volume):\n",
    "        ''' \n",
    "        Dimensionality reduction, keeps 'keep_volume' proportion of original variance\n",
    "        Type: collections.namedtuple ('dotted dictionary')\n",
    "        Examples of usage:\n",
    "        my_svd.volume(0.9).s - np.array: eigenvalues for 90% variance \n",
    "        my_svd.volume(0.8).f - dataframe: features for 80% variance\n",
    "        my_svd.volume(0.5).o - dataframe: occupations for 50% variance      \n",
    "        '''\n",
    "        dotted_dic = collections.namedtuple('dotted_dic', 's f o')\n",
    "        a1 = self.s.cumsum()\n",
    "        a2 = a1/a1[-1]\n",
    "        n_max = np.argmin(np.square(a2 - keep_volume))\n",
    "        cut_dic = dotted_dic(s= self.s[:n_max],f= self.f.iloc[:n_max], o= self.o.iloc[:n_max])\n",
    "        return cut_dic\n",
    "        \n",
    "\n",
    "\n",
    "## NLP UNDER DEVELOPMENT #################\n",
    "\n",
    "def drop_stopwords(wordvec,language='English'):\n",
    "    wv = np.array(wordvec)\n",
    "    stw = np.array(stopwords.words(language))\n",
    "    without_stopwords = wv[[not word in stw for word in wv]]\n",
    "    return without_stopwords\n",
    "\n",
    "def lemmatize(wordvec):\n",
    "    return [lemmatizer.lemmatize(word) for word in wordvec ]\n",
    "\n",
    "def nlp_prep(string):\n",
    "    wordvec = tokenizer.tokenize(string.lower())\n",
    "    return np.array(lemmatize(drop_stopwords(wordvec)))\n",
    "\n",
    "\n",
    "#def word_matrix(df_col):\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "title_vec = onet.socp_titles['Title'].apply(nlp_prep)\n",
    "onet.socp_titles['title_vec'] = title_vec\n",
    "onet.socp_titles['title_vec']\n",
    "\n",
    "tt = onet.socp_titles.set_index('SOCP_shave')[['title_vec']]\n",
    "keywords = np.array(list(set(tt['title_vec'].apply(list).sum())))\n",
    "df = pd.DataFrame(index = keywords, columns = tt.index)\n",
    "for socp,keyw in tt['title_vec'].to_dict().items():\n",
    "    df[socp].loc[keyw]=1\n",
    "sp = scipy.sparse.csr_matrix(df.fillna(0))\n",
    "\n",
    "\n",
    "\n",
    "##### Macro function\n",
    "\n",
    "    \n",
    "class Xyzzy:\n",
    "    '''\n",
    "    Xyzzy is a 'front-end for the back-end' and pipeline for the archetypes package. \n",
    "    Its purpose is to simplify the archetypal analysis of labor markets, where the US Census ACS/PUMS database \n",
    "    is used for mapping demographics with social and economic variables onto occupations, such \n",
    "    as ages and incomes of workers in a specific demographic, and where the O-net database \n",
    "    is used for mapping occupations onto variables, such as knowledge, skills or abilities. \n",
    "\n",
    "    Xyzzy was not designed to be a macro command language but the door can be kept open ;)\n",
    "    \n",
    "    #### Hyper parameters:\n",
    "    \n",
    "    state       : [str ] For census.data(state), e.g. 'California' or 'CA'. \n",
    "    state_cols  : [list] columns to include from census, default value ['WAGP','WKHP'] \n",
    "                    - wage & work hours per week. (definitions in census data dictionary)\n",
    "    fte         : [dict] Unless 'fte=False', 'fte' column (full-time wage equivalent) is added \n",
    "                    to the census data. Requires ['WAGP','WKHP']. \n",
    "                    Drops rows not fulfilling screeing requirements 'min_hours' and 'min_fte'\n",
    "                    Dictionary keys:\n",
    "                        'fulltime'  - work hours for fulltime        (default 40)\n",
    "                        'min_hours' - minimum nweekly hours required (default 15)\n",
    "                        'min_fte'   - minimum fte wage required      (default 0)\n",
    "    y_label     : [str] Name of census target variable/column\n",
    "    X_label     : [str] Name of O-net set of variables \n",
    "    socp_shave  : [int] occupational number length. Sets granularity of occupations.\n",
    "    norm        : [function] Function for normalizing the X-matrix, can be (but not restricted to) \n",
    "                    norm_dot  norm(vec) @ norm(vec) = 1 \n",
    "                    norm_stat norm(vec).mean = 0 ; norm(vec).std = 1\n",
    "                    norm_sum  norm(vec).sum = 1\n",
    "                    scale     norm(vec).min=0 ; norm(vec).max=1\n",
    "  \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self,\n",
    "            state,\n",
    "            state_cols = ['WAGP','WKHP'],\n",
    "            fte        = {'fulltime':40,'min_hours':15,'min_fte':0}, \n",
    "            y_label    = 'fte',\n",
    "            X_label    = 'Abilities',\n",
    "            socp_shave = 6,\n",
    "            norm       = norm_dot\n",
    "             ):\n",
    "        '''\n",
    "        MakeXy / Archetypes wizard. \n",
    "        '''\n",
    "        self.state = state\n",
    "        self.y_label = y_label\n",
    "        self.X_label = X_label\n",
    "        self.socp_shave = socp_shave\n",
    "        self.fte     = fte\n",
    "        \n",
    "        self.onet     = onet.matrix(self.X_label,socp_shave = socp_shave, norm = norm)\n",
    "        \n",
    "        census_cols = ['SOCP_shave'] + state_cols\n",
    "        self.cols   = census_cols\n",
    "        self.census   = census.data(self.state,  socp_shave = socp_shave)[self.cols]\n",
    "        if fte:\n",
    "            c0 = self.census.dropna()\n",
    "            c1 =  c0[\n",
    "                            c0['WKHP'] >= fte['min_hours']\n",
    "                            ]\n",
    "            c1['fte'] = fte['fulltime']*c1['WAGP']/c1['WKHP']\n",
    "            self.census = c1[c1['fte']>=fte['min_fte']] \n",
    "        self.make_Xy = MakeXy(self.census,self.onet,y_label = self.y_label)\n",
    "        self.X   = self.make_Xy.X\n",
    "        self.y   = self.make_Xy.y\n",
    "        self.Xy  = self.make_Xy.Xy\n",
    "        \n",
    "        self.svd = Svd(self.X)\n",
    "        \n",
    "        self.archetypes_dic ={}\n",
    "        \n",
    "        \n",
    "    def archetypes(self,n,norm=norm_dot):\n",
    "        if n not in self.archetypes_dic.keys():\n",
    "            self.archetypes_dic[(n,norm)] = Archetypes(self.X,n,norm=norm)\n",
    "        return self.archetypes_dic[(n,norm)]\n",
    "    \n",
    "\n",
    "    \n",
    "    def arch_dot(self,arch_n1, arch_n2, n_arch , kind = 'features'):\n",
    "        tr =   {'features'    : self.svd.f, \n",
    "                'occupations' : self.svd.o }\n",
    "        arch = {'features'    : self.archetypes(n_arch).f, \n",
    "                'occupations' : self.archetypes(n_arch).o.T }\n",
    "\n",
    "        artr_1 = arch[kind].iloc[arch_n1] @ tr[kind].T\n",
    "        artr_2 = arch[kind].iloc[arch_n2] @ tr[kind].T\n",
    "\n",
    "        return artr_1 @ artr_2\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
